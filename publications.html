<div class="container">
    <h3><u>Publications</u></h3>

    <!-- Publication Item 1 -->
    <div class="mb-4">
        <h5 class="fw-bold">Safer Policies via Affine Representations using Koopman Dynamics</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b>, Darshan Chudiwal, Greg Anderson, Abhinav Verma</em></p>
        <p class="text-muted mb-2">Submitted to NeurIPS 2025</p>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>

    <!-- Publication Item 2 -->
    <div class="mb-4">
        <h5 class="fw-bold">AutoSpec: Automating the Refinement of Reinforcement Learning Specifications</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b>, Đorđe Žikelić, Abhinav Verma</em></p>
        <p class="text-muted mb-2">Submitted to NeurIPS 2025</p>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>
    
    <!-- Publication Item 3 -->
    <div class="mb-4">
        <h5 class="fw-bold">Scaling Strategy, Not Compute: A Stand-Alone, Open-Source StarCraft II Benchmark for Accessible RL Research</h5>
        <p class="mb-1"><em>Sourav Panda, <b>Tanmay Ambadkar</b>, Shreyas Kale, Abhinav Verma, Jonathan Dodge</em></p>
        <p class="text-muted mb-2">Submitted to NeurIPS 2025</p>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>

    <!-- Publication Item 4 -->
    <div class="mb-4">
        <h5 class="fw-bold">MIXTAPE: Middleware for Interactive XAI with Tree-Based AI Performance Evaluation</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b>, Hayden Moore, Sourav Panda, Shreyash Kale, Connor Greenwell, Brianna Major, Aashish Chaudhary, Jonathan Dodge, Abhinav Verma and Brian Hu</em></p>
        <p class="text-muted mb-2">Simulation Interoperability Standards Organization (SISO) SIMposium, 2025</p>
        <div>
            <a href="https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fcdn.ymaws.com%2Fwww.sisostandards.org%2Fresource%2Fresmgr%2Fevents%2Fsiw%2F2025_siw%2Frevised_2025_abstracts.docx&wdOrigin=BROWSELINK"  target="_blank" rel="noopener noreferrer">Abstract Link</a>
        </div>
    </div>
    
    <!-- Publication Item 5 -->
    <div class="mb-4">
        <h5 class="fw-bold">MIXTAPE: Middleware for Interactive XAI with Tree-Based AI Performance Evaluation</h5>
        <p class="mb-1"><em>Brian Hu, Jonathan Dodge, Abhinav Verma, <b>Tanmay Ambadkar</b>, Sourav Panda, Sujay Koujalgi, Aashish Chaudhary, Brianna Major, and Bryon Lewis</em></p>
        <p class="text-muted mb-2">Simulation Interoperability Standards Organization (SISO) SIMposium, 2024</p>
        <div>
            <a href="https://cdn.ymaws.com/www.sisostandards.org/resource/resmgr%2Fevents/simposium_/2024_simposium_/2024_simposium_abstracts.pdf" target="_blank" rel="noopener noreferrer">Abstract Link</a>
        </div>
    </div>

    <!-- Publication Item 6 -->
    <div class="mb-4">
        <h5 class="fw-bold">AutoSpec: Automating the Refinement of Reinforcement Learning Specifications</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b></em></p>
        <p class="text-muted mb-2">ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), 2024</p>
        <div>
            <a href="https://pldi24.sigplan.org/details/pldi-2024-src/11/AutoSpec-Automating-the-Refinement-of-Reinforcement-Learning-Specifications" target="_blank" rel="noopener noreferrer">Poster</a>
        </div>
    </div>

    <!-- Publication Item 7 -->
    <div class="mb-4">
        <h5 class="fw-bold">Optimizing Operational Costs in Combined Heat and Power Integrated District Heating Systems: A Reinforcement Learning Approach</h5>
        <p class="mb-1"><em>Saranya Anbarasu, <b>Tanmay Ambadkar</b></em></p>
        <p class="text-muted mb-2">SimBuild, 2024</p>
        <div>
            <a href="https://publications.ibpsa.org/conference/paper/?id=simbuild2024_2147" target="_blank" rel="noopener noreferrer">Link</a>
        </div>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>

    <!-- Publication Item 8 -->
    <div class="mb-4">
        <h5 class="fw-bold">A Simple Fast Resource-efficient Deep Learning for Automatic Image Colorization</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b>, Jignesh S. Bhatt</em></p>
        <p class="text-muted mb-2">Color and Imaging Conference (CIC), 2023</p>
        <div>
            <a href="https://library.imaging.org/cic/articles/31/1/23" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/TanmayAmbadkar/ImageColorNet-Residual-Colorization"target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://doi.org/10.2352/CIC.2023.31.1.24" target="_blank" rel="noopener noreferrer">DOI</a>
        </div>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>

    <!-- Publication Item 9 -->
    <div class="mb-4">
        <h5 class="fw-bold">Discrete Sequencing for Demand Forecasting: A novel data sampling technique for time series forecasting</h5>
        <p class="mb-1"><em>N. Menon, S. Saboo, <b>T. Ambadkar</b> and U. Uppili</em></p>
        <p class="text-muted mb-2">International Conference on Intelligent Data Science Technologies and Applications (IDSTA), 2022</p>
        <div>
            <a href="https://ieeexplore.ieee.org/document/9923044" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://doi.org/10.1109/IDSTA55301.2022.9923044" target="_blank" rel="noopener noreferrer">DOI</a>
        </div>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>

    <!-- Publication Item 10 -->
    <div class="mb-4">
        <h5 class="fw-bold">Deep reinforcement learning approach to predict head movement in 360° videos</h5>
        <p class="mb-1"><em><b>Tanmay Ambadkar</b>, Pramit Mazumdar</em></p>
        <p class="text-muted mb-2">Proc. IS&T Int’l. Symp. on Electronic Imaging: Image Processing: Algorithms and Systems, 2022</p>
        <div>
            <a href="https://library.imaging.org/ei/articles/34/10/IPAS-367" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://github.com/TanmayAmbadkar/DRL-FOV"target="_blank" rel="noopener noreferrer">Code</a>
            <a href="https://doi.org/10.2352/EI.2022.34.10.IPAS-367" target="_blank" rel="noopener noreferrer">DOI</a>
        </div>
        <details>
            <summary><strong>Abstract</strong></summary>
            <div style="padding-left: 1rem; margin-top: 0.5rem;">
                <p>
                <strong>The Problem:</strong> Manually designing a perfect reward function to guide an RL agent is extremely difficult and a major barrier for non-experts. A slightly flawed specification can lead to completely wrong or unexpected behavior.
                </p>
                <p>
                <strong>My Approach (AutoSpec):</strong> I developed a framework called <strong>AutoSpec</strong> that allows a user to provide an initial, high-level, and potentially imperfect specification. The agent then autonomously refines and corrects this specification during training by identifying and resolving inconsistencies, leading to better task performance without requiring constant human intervention. This makes RL more accessible to domain experts who need the technology but are not RL specialists.
                </p>
            </div>
        </details>
    </div>
    <hr>
</div>
